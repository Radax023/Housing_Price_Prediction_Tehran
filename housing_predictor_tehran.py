# -*- coding: utf-8 -*-
"""Housing_Predictor_Tehran.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13TGZQP8uW9ni-3Y8FXgtCOzpT5w9f2KM
"""

from pathlib import Path
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

housing = pd.read_csv("housePrice.csv")

housing.head()

housing.info()

housing.dropna(axis=0,inplace=True)
housing['Area'] = pd.to_numeric(housing['Area'],errors='coerce')
housing["Area"] = housing['Area'].fillna(housing['Area'].median())

housing.info()

"""Tried to Use The forrent usd value, but failed, we just predict IRR for back then"""

import requests

url = "https://api.alanchand.com/?type=currencies&token=W9vnxID7CUbCSlHLoac7"
response = requests.get(url).json()

usd_sell = response["usd"]["sell"]

#housing["Price"] = housing["Price(USD)"] * usd_sell
housing = housing.drop(['Price(USD)'],axis=1)

housing.hist(bins=50, figsize=(12, 8))
plt.show()

"""Removin Outliers"""

plt.boxplot(housing['Area'])
plt.show()

housing = housing[housing["Area"] <= 200]

plt.boxplot(housing['Price'])
plt.show()

housing = housing[housing["Price"] <= 1.3 * 1e11]

corr_matrix = housing.corr(numeric_only=True)
corr_matrix["Price"].sort_values(ascending=False)

"""We can Add two more Attributes, and assume the value of each neighbor by the Average Area in that Neighbor. We can later try both Area per Address and Price per address and see which one gives better result."""

housing['Total_Amenities'] = housing['Parking'].astype(int) + housing['Elevator'].astype(int) + housing['Warehouse'].astype(int)
housing['Area_per_Address'] = housing.groupby('Address')['Area'].transform('mean')

corr_matrix = housing.corr(numeric_only=True)
corr_matrix["Price"].sort_values(ascending=False)

from pandas.plotting import scatter_matrix
attributes = ["Area", "Room", "Price", "Total_Amenities", "Area_per_Address"]
scatter_matrix(housing[attributes], figsize=(12, 8))
plt.show()

"""We make Sure to delete duplicated Datas to avoid data leakage"""

housing = housing.drop_duplicates()
housing.info()

X = housing.drop(['Price', 'Address', 'Parking', 'Warehouse', 'Elevator'], axis=1)
y = housing['Price'].copy()
X.describe()

y.describe()

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)

X_train.info()

X_test.info()

"""We try to have two approaches(Pipelines) one for models that needs data to be scaled, and another for models that don't. we try three models, LinearRegression which needs scaling, DecisionTreeRegressor and RandomForestRegressor which don't"""

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score

pipeline_scaled = Pipeline([("scaler", StandardScaler()), ("lin_reg", LinearRegression())])

pipeline_scaled.fit(X_train, y_train)
pred_scaled = pipeline_scaled.predict(X_test)
r2_LR = r2_score(y_test, pred_scaled)

pipeline_tree = Pipeline([("tree_reg", DecisionTreeRegressor(random_state=42))])

pipeline_tree.fit(X_train, y_train)
pred_tree = pipeline_tree.predict(X_test)
r2_DT = r2_score(y_test, pred_tree)

pipeline_rf = Pipeline([("forest_reg", RandomForestRegressor(random_state=42))])

pipeline_rf.fit(X_train, y_train)
pred_rf = pipeline_rf.predict(X_test)
r2_RF = r2_score(y_test, pred_rf)

print(f"Linear Regression R2 Score: {r2_LR:.4f}")
print(f"Decision Tree R2 Score: {r2_DT:.4f}")
print(f"Random Forest R2 Score: {r2_RF:.4f}")

"""Random Forst Shows the best performance"""

from sklearn.model_selection import RandomizedSearchCV

param_grid = {
    'forest_reg__n_estimators': [50, 100, 200, 300],
    'forest_reg__max_depth': [None, 10, 20],
    'forest_reg__max_features': ['auto', 'sqrt'],
    'forest_reg__min_samples_split': [2, 5, 10],
    'forest_reg__min_samples_leaf': [1, 2, 4]
}

rf_pipeline = Pipeline([("forest_reg", RandomForestRegressor(random_state=42))])

random_search = RandomizedSearchCV(
    estimator=rf_pipeline,
    param_distributions=param_grid,
    n_iter=30,
    cv=5,
    scoring='r2',
    random_state=42,
    n_jobs=-1
)

random_search.fit(X_train, y_train)
best_model = random_search.best_estimator_

pred_best = best_model.predict(X_test)
r2_best = r2_score(y_test, pred_best)

print("\nBest Random Forest Model:", random_search.best_params_)
print(f"Best Random Forest R2 Score: {r2_best:.4f}")

pred_best[:10]

y_test[:10]

mape = np.mean(np.abs((y_test - pred_best) / y_test)) * 100
print(f"MAPE: {mape:.2f}%")

